import pandas as pd
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.linear_model import Perceptron
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

data_path = "C:\\Users\\vvmad\\Downloads\\5th\\ML\\HM7 - Java\\java_cc_embed_data.csv"
df = pd.read_csv(data_path)

X = df.iloc[:, :-1]
y = df.iloc[:, -1]

def q7_a2(X, y):
    perceptron = Perceptron()
    params_perceptron = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1]}
    rs_perceptron = RandomizedSearchCV(perceptron, param_distributions=params_perceptron, n_iter=10, cv=5)
    rs_perceptron.fit(X, y)

    mlp = MLPClassifier()
    params_mlp = {'alpha': [0.0001, 0.001, 0.01, 0.1], 'hidden_layer_sizes': [(100,), (50, 50), (30, 30, 30)]}
    rs_mlp = RandomizedSearchCV(mlp, param_distributions=params_mlp, n_iter=10, cv=5)
    rs_mlp.fit(X, y)

    return rs_perceptron.best_params_, rs_mlp.best_params_

def q7_a3(X, y):
    classifiers = {
        'SVM': SVC(),
        'DecisionTree': DecisionTreeClassifier(),
        'RandomForest': RandomForestClassifier(),
        'CatBoost': CatBoostClassifier(verbose=0),
        'AdaBoost': AdaBoostClassifier(),
        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
        'NaiveBayes': GaussianNB()
    }

    results = {}
    for name, clf in classifiers.items():
        scores = cross_val_score(clf, X, y, cv=5)
        results[name] = scores.mean()

    return results

best_params_perceptron, best_params_mlp = q7_a1(X, y)
results_classifiers = q7_a2(X, y)

print("Best Params for Perceptron:", best_params_perceptron)
print("Best Params for MLP:", best_params_mlp)
print("Classifier Results:")
for clf, score in results_classifiers.items():
    print(f"{clf}: {score:.4f}")
